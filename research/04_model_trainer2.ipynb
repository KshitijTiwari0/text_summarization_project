{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projectFiles\\\\text_summarization_project\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projectFiles\\\\text_summarization_project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from text_summarizer.constants import *\n",
    "from text_summarizer.utils.common import read_yaml, create_directories\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: str\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: int\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"D:\\\\projectFiles\\\\text_summarization_project\\\\config\\\\config.yaml\"\n",
    "PARAMS_FILE_PATH = \"D:\\\\projectFiles\\\\text_summarization_project\\\\params.yaml\"\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        # Convert the file paths to pathlib.Path objects\n",
    "        config_filepath = Path(config_filepath)\n",
    "        params_filepath = Path(params_filepath)\n",
    "\n",
    "        # Print the paths when initializing\n",
    "        print(\"Config File Path:\", config_filepath)\n",
    "        print(\"Params File Path:\", params_filepath)\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        root_dir = Path(config.root_dir)\n",
    "\n",
    "        create_directories([root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            model_ckpt=config.model_ckpt,\n",
    "            num_train_epochs=params.num_train_epochs,\n",
    "            warmup_steps=params.warmup_steps,\n",
    "            per_device_train_batch_size=params.per_device_train_batch_size,\n",
    "            weight_decay=params.weight_decay,\n",
    "            logging_steps=params.logging_steps,\n",
    "            evaluation_strategy=params.evaluation_strategy,\n",
    "            eval_steps=params.evaluation_strategy,\n",
    "            save_steps=params.save_steps,\n",
    "            gradient_accumulation_steps=params.gradient_accumulation_steps\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Use the specified model checkpoint\n",
    "        model_checkpoint = \"google/pegasus-xsum\"\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_tf=True).to(device)\n",
    "\n",
    "        # Initialize the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n",
    "\n",
    "        # loading data\n",
    "        dataset_samsum_pt = load_from_disk(str(self.config.data_path))\n",
    "\n",
    "        dataset_samsum_pt = dataset_samsum_pt.map(lambda example: {key: example[key][:10] for key in example})\n",
    "\n",
    "        # Set the output directory to the desired path\n",
    "        output_dir = \"D:\\\\projectFiles\\\\text_summarization_project\\\\model_data\"\n",
    "\n",
    "        trainer_args = TrainingArguments(\n",
    "            output_dir=self.config.root_dir,\n",
    "            num_train_epochs=1,\n",
    "            warmup_steps=50,\n",
    "            per_device_train_batch_size=1,  # Reduce this value\n",
    "            per_device_eval_batch_size=1,   # Reduce this value\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=10,\n",
    "            evaluation_strategy='steps',\n",
    "            eval_steps=50,\n",
    "            save_steps=1e6,\n",
    "            gradient_accumulation_steps=16\n",
    "        )\n",
    "\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model_pegasus, args=trainer_args,\n",
    "            tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "            train_dataset=dataset_samsum_pt[\"train\"],\n",
    "            eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Save model to the specified directory\n",
    "        model_pegasus.save_pretrained(output_dir)\n",
    "        # Save tokenizer to the specified directory\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Assuming the rest of the code for configuration is the same as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config File Path: D:\\projectFiles\\text_summarization_project\\config\\config.yaml\n",
      "Params File Path: D:\\projectFiles\\text_summarization_project\\params.yaml\n",
      "[2024-01-01 18:12:40,513: INFO: common:yaml file: D:\\projectFiles\\text_summarization_project\\config\\config.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-01 18:12:40,550: INFO: common:yaml file: D:\\projectFiles\\text_summarization_project\\params.yaml loaded successfully]\n",
      "[2024-01-01 18:12:40,559: INFO: common:created directory at: artifacts]\n",
      "[2024-01-01 18:12:40,560: INFO: common:created directory at: artifacts\\model_trainer]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AssignVariableOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[96103,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AssignVariableOp] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_trainer_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_model_trainer_config()\n\u001b[0;32m      4\u001b[0m     model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(config\u001b[38;5;241m=\u001b[39mmodel_trainer_config)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use the specified model checkpoint\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-xsum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m model_pegasus \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3673\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3671\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[1;32m-> 3673\u001b[0m     model, loading_info \u001b[38;5;241m=\u001b[39m \u001b[43mload_tf2_checkpoint_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   3675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3676\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   3677\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m   3678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instructions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3681\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:464\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[1;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     tf_model(tf_inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Make sure model is built\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m \u001b[43mload_tf_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_checkpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_tf2_model_in_pytorch_model(\n\u001b[0;32m    467\u001b[0m     pt_model, tf_model, allow_missing_keys\u001b[38;5;241m=\u001b[39mallow_missing_keys, output_loading_info\u001b[38;5;241m=\u001b[39moutput_loading_info\n\u001b[0;32m    468\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:882\u001b[0m, in \u001b[0;36mload_tf_weights\u001b[1;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     load_function \u001b[38;5;241m=\u001b[39m load_tf_weights_from_h5\n\u001b[1;32m--> 882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prefix\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\modeling_tf_utils.py:978\u001b[0m, in \u001b[0;36mload_tf_weights_from_h5\u001b[1;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((symbolic_weight, array))\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# Load all the weights\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_value_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# Compute the missing and unexpected layers\u001b[39;00m\n\u001b[0;32m    981\u001b[0m missing_layers\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(symbolic_weights_names \u001b[38;5;241m-\u001b[39m saved_weight_names_set))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\src\\backend.py:4313\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   4311\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[0;32m   4312\u001b[0m         value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39mdtype_numpy(x))\n\u001b[1;32m-> 4313\u001b[0m         \u001b[43m_assign_value_to_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\keras\\src\\backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[1;34m(variable, value)\u001b[0m\n\u001b[0;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[0;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[1;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AssignVariableOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[96103,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AssignVariableOp] name: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
